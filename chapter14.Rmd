---
title: "BDA: Chapter 14 exercices"
author: "OS"
output: html_document
---

# Exercice 5

Can't have a pertinent discussion until having thougroly read chapter 8.

# Exercice 6

Too tedious. Most essential conditions is that $n > k$ where $n$ is the number
of observations and $k$ is the number of linearly idependent covariates. This
is required so that $X^TX$ has full rank and is thus invertablle. If this
condition is not respected then the variance estimator $\sigma^2$ is also
undefined.

# Exercice 7

I cheated and this is essentially taken from the solutions page.

So we have
\begin{equation}
p(\tilde{y}, \beta|\sigma, y) =
p(\tilde{y}|\beta, \sigma, y) p(\beta | \sigma, y)
\end{equation}

The first term is the prosterior predictive distribution, which has essentially
the same form as the likelihood, i.e., multivariate normal, and the latter is
the posterior distribution of $\beta$, also multivariate normal. Now, product
of the two is an exponential of a quadratic form in something, but I'm not
convinced that it is necessairy quadratic in $(\tilde{y}, \beta)$. (Although
I realise that it is must be true.)

I found it useful to first write what we're looking for. If $(\tilde{y}, \beta)$
are jointly normal then they log density proportinal to

\begin{equation}
\def\e1{\tilde{y} - X\beta}
\def\e2{\beta - \hat\beta}
\begin{pmatrix}
\e1\\
\e2
\end{pmatrix}^T
\begin{pmatrix}
I\sigma^2 & A^T\\
A & V*\beta\sigma^2
\end{pmatrix}
\begin{pmatrix}
\e1\\
\e2
\end{pmatrix}\\
= \e1^TI\sigma^2\e2 + \e2^TA\1e + \e1^TA^T\e2 + \e2^T(V*\beta\sigma^2)\e2
\end{equation}

Now on the other side we get.

\begin{align}
p(\tilde{y}|\beta, \sigma, y)p(\beta | \sigma, y)
&= N(\tilde{y}|\hat{\beta} X, I\hat{\sigma}^2) N(\beta|\hat{\beta},V\_\beta\hat\sigma^2)\\
&\propto exp(-\frac{1}{2}(\tidle{y} - \hat\beta X)^T(I\hat\sigma^2)^-1(\tidle{y} - \hat\beta X) +
(\tidle{y} - \beta X)^T(I\sigma^2)^-1(\tidle{y} - \beta X))
\end{align}

Now the first and the last tem coinside perfeclty with the previous statement.
And since theree are no other terms I conclude that $A = 0$. That is
$\tilde y$ is idependent to $\beta$ conditionally on $(\sigma, \y)$. Which we
kind of should have gussed.

# Exercice 8

Let us take a look at a model defined with the following terms

\begin{equation}
y*\* = \begin{pmatrix}
y\\
\beta_0
\end{pmatrix},\quad
X*_ = \begin{pmatrix}
X\\
I*k
\end{pmatrix},\quad
\Sigma*_ = \begin{pmatrix}
\Sigma*y & 0 \\
0 & \Sigma*\beta
\end{pmatrix}.
\end{equation}

The prior is unchanged, but the likelihood is now
\begin{equation}
N(y*\*|X*_\beta, \Sigma\__)
\end{equation}

Now, insted of developping ths specific case might as well develop the general
case with arbitrary variance parameters, that is $p(y|\beta, \phi) = N(y |
X\beta, diag(\exp(W\phi)))$. Where $\phi$ is the variance parametrs and $W$ is
the variance structure. For instance the usual case with constant variance we
would have $\phi = 2 \log(\sigma)$ and $W = \textbf{1}$ (vector of ones).
For convenience we note that $\Sigma = diag(\exp{(W\phi)}^{-1})$

So with all that, and by completing the square as previously but this time the
quadratic form uses the matrix $\Sigma$ instead of an identity matrix. The
part of the posterior within the exponatial is

\begin{equation}
(\beta - (X^T\Sigma X)^{-1}X^T\Sigma y) (X^T\Sigma X)^{-1}
(\beta - (X^T\Sigma X)^{-1}X^T\Sigma y) +
y^T (\Sigma^{-1} - \Sigma^TX(X^T\Sigma X)^{-1}X^T\Sigma) y
\end{equation}

The rest of the posterior is equal to

\begin{equation}
(2\pi)^{\frac{k}{2}} |\Sigma^{-1}|^{-1}
\exp(\text{The expression above}) \prod \phi_i^2
\end{equation}

Conditionally on $\phi$ and $y$, this a multivariate normal distribution with
mean $(X^T\Sigma X)^{-1}X^T\Sigma y$ and $X^T\Sigma X$ variance-covariance
matrix. Nice.

Now what's left ?

$$
(2\pi)^{\frac{k}{2}} |\Sigma^{-1}|^{-1}
    \exp{y^T (\Sigma^{-1} - \Sigma^TX(X^T\Sigma X)^{-1}X^T\Sigma) y}
    \prod \phi_i^2
$$

Now the inverse determiant simplifies to

$$
|\Sigma^{-1}|^{-1} = |diag(\exp{(W\phi)})|^{-1}\\
    = \prod(\exp{\frac{1}{W\phi}})^{-1} = \exp(-1\sum \frac{1}{W\phi})
$$

Finally

$$
(2\pi)^{\frac{k}{2}} \exp(-1\sum \frac{1}{W\phi}) \prod \phi_i^2
    \exp{y^T (\Sigma^{-1} - \Sigma^TX(X^T\Sigma X)^{-1}X^T\Sigma) y}.
$$

Hmm, does this look like a Scaled inverse $\Chi^2$ ? Dunno. Kind of, since
we have something like $x^n exp(1/x)$.

Anyway, the initial question was to proove that adding pseudo-observatons
is equivalent to setting a priror.

To do that let us start with $\beta$. Overall the log posterior has this
form :

$$
(\beta - \hat\beta)^T (X^T\Sigma_yX)^{-1} (\beta - \hat\beta) +
    (\beta - \beta_0)^T \Sigma_\beta^{-1} (\beta - \beta_0).
$$

The first term is the non-infored posterior, the second is the prior on
$\beta$ which is also normal.

This is equal to

$$
(\beta - (\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})^{-1}((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0))
(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})
(\beta - (\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})^{-1}((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0))\\
- ((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0)^T(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})^{-1}((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0)\\
+ \beta_0^T\Sigma_\beta^{-1}\beta_0 + \hat\beta(X^T\Sigma_yX)^{-1}\hat\beta.
$$

The important part is that this means that the posterior distribution
is $N(\beta|(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})^{-1}((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0),(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1}))$
Which loks reasonable, as the expected value is a weighted average of the
ininformed estimator and the prior mean.

However, i'm having trouble matching this to the previous expression,
it is not evident that means and variance-covariances matricies match up :

$$
(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1})^{-1}((X^T\Sigma_yX)^{-1}\hat\beta + \Sigma_\beta^{-1}\beta_0) = (X_*^T\Sigma_*X_*)^{-1}X_*^T\Sigma_*y_*),
(\Sigma_\beta^{-1} + (X^T\Sigma_yX)^{-1}) = X_*^T\Sigma_*X_*
$$

# Exercice 9

## a

Posterior density of $\beta$ with non-informative prior is normal. We multipy
that by the prior on $\beta$, that is, $\prod_j exp(-\lambda |\beta_j|)$.
Note that this also equals to $exp(-\lambda \sum_j |beta_j|)$. We can thus
only write the part of the posterior within the exponent.

$$
log(\beta|y, \sigma) \propto
    (\beta - \hat\beta)^T (V_\beta\sigma^2)^{-1} (\beta - \hat\beta) +
    -\lambda \sum_j |\beta_j|
$$

## b

If we treat $\beta$ as one dimential, that is, a scalar the expression
above simplifies to 

$$
log(\beta|y, \sigma) \propto
    \frac{(\beta - \hat\beta)^2}{(v_\beta\sigma^2)^{-1}}
    -\lambda |\beta|
$$

Finding the posterior mode is equivalent to finding the maximum of the above
expression (because $\log$ is monotenous and increasing). To find the maximum
we look for the roots of the derivative. The derivative exists everywhere
except at $0$ where $|x|$ is not smooth.

$$
    2 v_\beta\sigma^2 (\beta - \hat\beta) = \lambda \text{sign}(\beta)\\
    \beta - \hat\beta = \lambda \frac{\text{sign}(\beta)}{2 v_\beta\sigma^2}\\
    \beta = \hat\beta + \lambda \frac{\text{sign}(\beta)}{2 v_\beta\sigma^2}\\
$$

This seems very close to the truth since it says that the Lasso estimator
of $\beta$ is the usual (least squares) estimator plus some term depending on
regularization parameter $\lambda$. Moreover for $\lambda = 0$ the second term
disappears. However, I would have expeted the term to be pulling the estimate
towards zero, and this is not the behavior we seem to observe. Perhaps I
lost a sign somewhere.

## c

When can find the above derivative for the matrix version as well.

$$
\frac{\partial \log{\beta}}{\partial \beta} = 
    2 (V_\beta\sigma^2)^{-1} (\beta - \hat\beta) +
    -\lambda \text{Sign}(\beta_j)
$$

And the root is at :

$$
\beta =
\hat\beta + \frac{\lambda}{2} (V_\beta\sigma^2)\text{Sign}(\beta_j).
$$

If we take the $i$th coefficent of $\beta$, we get

$$
\beta_i =
    \hat\beta_i + \frac{\lambda}{2} (V_\beta\sigma^2)_{i, \dot} \text{Sign}(\beta_j).
$$

Therefore, $\beta_i$ is not necessairy pulled towards $0$ and the adjustement
depends on it's correlation to other $\beta$s and on all of the signs.


# Exercice 10

This exercice call for using a dataset in "in an application
area of interest to you, with many predictors". I use a dataset of
automobile bodily injuries.

```{r}
library(tidyverse)
library(insuranceData)
library(rstan)
data(AutoBi)
AutoBi <- as_tibble(AutoBi) %>%
    mutate(
        CASENUM = NULL,
        ATTORNEY = factor(ifelse(ATTORNEY == 1, "Yes", "No")),
        CLMSEX = factor(ifelse(CLMSEX == 1, "Male", "Female")),
        MARITAL = case_when(
            MARITAL == 1 ~ "Married",
            MARITAL == 2 ~ "Single",
            MARITAL == 3 ~ "Widowed",
            MARITAL == 4 ~ "Separated"
        ),
        MARITAL = as.factor(MARITAL),
        CLMINSUR = case_when(
            CLMINSUR == 1 ~ "Yes",
            CLMINSUR == 2 ~ "No",
            CLMINSUR == 3 ~ "Not applicable",
        ),
        CLMINSUR = as.factor(CLMINSUR),
        SEATBELT = case_when(
            SEATBELT == 1 ~ "Yes",
            SEATBELT == 2 ~ "No",
            SEATBELT == 3 ~ "Not applicable"
        ),
        SEATBELT = as.factor(SEATBELT),
    )
summary(AutoBi)
```

As a baseline let us estimate the usual linear model.

```{r}
formula <- log(LOSS) ~ ATTORNEY + CLMSEX + MARITAL + CLMINSUR + SEATBELT + CLMAGE
auto_bi_lm <- lm(formula, data = AutoBi)
summary(auto_bi_lm)
```

First, I noticed that the response variable is very asymetric, with a pretty
long right tail. So I choose to model the log of the response.
Also, buch of variables are not significant,

## a

First estimate Lasso problem

```{r}
library(glmnet)
library(broom)

mf_autobi <- model.frame(formula, AutoBi)
auto_bi_glmnet <- cv.glmnet(
    model.matrix(mf_autobi, AutoBi)[, -1],
    model.response(mf_autobi)
)
plot(auto_bi_glmnet)
coef(auto_bi_glmnet, s = "lambda.min")
min_lambda <- tidy(auto_bi_glmnet) %>%
    filter(estimate == min(estimate)) %>%
    pull(lambda)

moda <- function(x, ...) {
    d <- density(x, ...)
    d$x[which.max(d$y)]
}
```

Now, we repeat the same process but on bootstraped samples.

```{r}
boot_pars <- replicate(100, {
    n <- nrow(AutoBi)
    idx <- sample.int(n, size = n, replace = TRUE)
    mf <- model.frame(formula, AutoBi[idx, ])
    abg <- cv.glmnet(
        model.matrix(mf, AutoBi[idx, ])[, -1],
        model.response(mf)
    )
    min_lambda <- tidy(abg) %>%
        filter(estimate == min(estimate)) %>%
        pull(lambda)
    c(
        as.vector(coef(abg, s = "lambda.min")),
        sigma = NA,
        lambda = min_lambda
    )
}) %>% t()
```


Now we can, visualize the distribution of the lambdas. 

```{r}
hist(boot_pars[, "lambda"])
```

The mode of this distribution is `r moda(boot_pars[, "lambda"])`.

## c

Now to estimate a fully Bayesian Lasso model.

First, a small note. By accident, I fit a model that has a completely
flat prior (not on $\sigma^{-2}$, just $\propto 1$). This resulted in a model
that seemed to completely align with the usual least squares estimates.

I had trouble with the $\lambda$ parameter; I tried a flat prior, or some
weekly informative one like $\lambda^{-2}$, or a half-cauchy. It didn't work.
At best $\lambda$ estimate was extremely variable with huge $\hat R$ and tiny
effective sample sizes, at worst the same was true for all parameters. At the
end of the day I use the fact that effectively $\lambda$ has a limited range,
under a certain value the model is the least squares estimate, above certain
value only the intercept is kept. I cheat and look at `glmnet` lasso to get the
range.

```{r}
lasso_lambda_stan <- "
data {
    int<lower=0> K;
    int<lower=0> N;
    vector[N] y;
    matrix[N,K-1] x;
}
parameters {
    real beta0;
    vector[K-1] beta;
    real<lower=0> sigma;
    real llambda;
}
transformed parameters {
    real<lower=0> lambda = exp(llambda);
}
model {
    // Likelihood
    target += normal_lpdf(y | beta0 + x*beta, sigma);
    // Sigma prior
    target += -2 * sigma;
    // Beta prior
    target += lambda * sum(fabs(beta));
    // lambda prior
    llambda ~ uniform(-7, 0);
}
"
fit2 <- stan(
    model_code = lasso_lambda_stan,
    data = list(
        N = nrow(model.matrix(mf_autobi, AutoBi)),
        K = ncol(model.matrix(mf_autobi, AutoBi)),
        y = model.response(mf_autobi),
        x = scale(model.matrix(mf_autobi, AutoBi)[, -1])
    ),
    chains = 4,
    warmup = 5e3,
    iter = 10e3,
    cores = 1
)
```

It is actually kind of tricky to calculate the mode from a sample. I'm going to
just use the `density` estimation.

```{r}
simuls2 <- rstan::extract(fit2)
c(
    beta0 = moda(simuls2[["beta0"]]),
    apply(simuls2[["beta"]], 2, moda),
    sigma = moda(simuls2[["sigma"]]),
    lambda = moda(simuls2[["lambda"]])
)
```

## c.2

I also fit a lasso model but with $\lambda$ parameter fixed to the value chosen
by cross-validation.

```{r}
lasso_lm_stan <- "
data {
    int<lower=0> K;
    int<lower=0> N;
    vector[N] y;
    matrix[N,K-1] x;
    real<lower=0> lambda_min;
}
parameters {
    real beta0;
    vector[K-1] beta;
    real<lower=0> sigma;
}
transformed parameters {
    real<lower=0> lambda = lambda_min;
}
model {
    // Likelihood
    target += normal_lpdf(y | beta0 + x*beta, sigma);
    // Sigma prior
    target += -2 * sigma;
    // Beta prior
    target += lambda * sum(fabs(beta));
}
"
fit1 <- stan(
    model_code = lasso_lm_stan,
    data = list(
        N = nrow(model.matrix(mf_autobi, AutoBi)),
        K = ncol(model.matrix(mf_autobi, AutoBi)),
        y = model.response(mf_autobi),
        x = scale(model.matrix(mf_autobi, AutoBi)[, -1]),
        lambda_min = min_lambda
    ),
    chains = 4,
    warmup = 1e3,
    iter = 2e3,
    cores = 1
)
```

## d

Now time to compare the results.

```{r}
simuls1 <- rstan::extract(fit1)
compar_coefs <- rbind(
    bootstrap = apply(boot_pars, 2, function(x) if (any(is.na(x))) NA else moda(x)),
    fixed_lambda = c(
        beta0 = moda(simuls1[["beta0"]]),
        apply(simuls1[["beta"]], 2, moda),
        sigma = moda(simuls1[["sigma"]]),
        lambda = moda(simuls1[["lambda"]])
    ),
    full_bayes = c(
        beta0 = moda(simuls2[["beta0"]]),
        apply(simuls2[["beta"]], 2, moda),
        sigma = moda(simuls2[["sigma"]]),
        lambda = moda(simuls2[["lambda"]])
    )
) %>% t()
ncoef <- length(coef(auto_bi_glmnet, s = "lambda.min"))
coef_names <- rownames(coef(auto_bi_glmnet, s = "lambda.min"))
rownames(compar_coefs)[seq_len(ncoef)] <- coef_names
compar_coefs
```

Most of the coefficients are similar. The noticable differences are between
bootstraped and Bayesian estimations. The largest seems to be for the lambda
coefficient which is rather different for the full bayesian approach.

```{r}
ggplot() +
    aes() +
    geom_histogram(aes(x = boot_pars[, "lambda"]), color = "red", alpha = 0.5) +
    geom_histogram(aes(x = simuls2[["lambda"]]), )

compar_coefs <- rbind(
    bootstrap = apply(boot_pars, 2, function(x) if (any(is.na(x))) NA else moda(x)),
    fixed_lambda = c(
        beta0 = moda(simuls1[["beta0"]]),
        apply(simuls1[["beta"]], 2, moda),
        sigma = moda(simuls1[["sigma"]]),
        lambda = moda(simuls1[["lambda"]])
    ),
    full_bayes = c(
        beta0 = moda(simuls2[["beta0"]]),
        apply(simuls2[["beta"]], 2, moda),
        sigma = moda(simuls2[["sigma"]]),
        lambda = moda(simuls2[["lambda"]])
    )
) %>% t()
```


On to the task

# Exercice 11

First, let's write down the likelihood with $u_i$. We can take a fixed $i$ since
the whole likelihood of $(x, y)$ is just the product of likelihoods of
$(x_i, y_i)$.

\begin{equation}
p(x_i, y_i|a, b) = p(x_i, y_i|u_i, v_i) p(v_i|u_i,a,b) p(u_i)
\end{equation}

The proper way to find this is by writing the density of all the quantities,
conditional on $(a,b)$.

\begin{align}
p(x_i, y_i, u_i, v_i | a,b) &= p(x_i, y_i | u_i, v_i, a, b) p(u_i, v_i | a,b) \\
    &= p(x_i, y_i | u_i, v_i) p(v_i | u_i, a,b) p( u_i | a,b) \\
    &= p(x_i, y_i | u_i, v_i) p(v_i | u_i, a,b) p(u_i)
\end{align}

I failed to find an easy way to integrate this. Instead I suggest viewing
the $(u_i, v_i)$ as a multivariate normal with $(\mu, a + b\mu)$ mean vector,
and $\begin{pmatrix}\tau^2 & b\tau\\b\tau&b^2\tau^2\end{pmatrix}$.

This would mean that the likelihood can be re-written as product of two normals.
Now, I didn't bother checking (read : couldn't proove it on my own but didn't
want to look it up ether), but I think that a multivarate normal with 
multivaraite normal mean is also multivaraite normal with variance matrix
that is the sum of the two. In our case this means that we can simply write

$$
p(x_i, y_i | a, b) =
N((\mu, a + b\mu), \Sigma + \begin{pmatrix}\tau^2 & b\tau\\b\tau&b^2\tau^2\end{pmatrix}).
$$

## b 

The likelihood looks rather like a normal model, but $(a,b)$ are not the mean
parameter. Also, both values can take positive or negative values. So my first
idea is to just use a flat prior, $\propto 1$. Another way to go would be to use
a distribution with large variance, such as the Student distribution, or Cauchy.

# Exercice 12

## a

```{r}
dogs <- tibble(
    mass = c(31.2, 24.0, 19.8, 18.2, 9.6, 6.5, 3.2),
    surface = c(10750, 8805, 7500, 7662, 5286, 3724, 2423),
    metabolism = c(1113, 982, 908, 842, 626, 430, 281),
)

slf_stan <- c("
data {
    int<lower=0> N;
    vector[N] lmass;
    vector[N] lmetabolism;
}
transformed data {
    matrix[N, 2] y;
    y[,1] = lmass;
    y[,2] = lmetabolism;
}
parameters {
    real mu;
    real a;
    real b;
    real<lower=0> sigma;
    real<lower=0> tau;
}
transformed parameters {
    vector[2] Mu;
    Mu[1] = mu;
    Mu[2] = a*mu + b;

    matrix[2,2] Sigma;
    Sigma[1,1] = sigma^2;
    Sigma[2,2] = sigma^2;
    Sigma[1,2] = 0;
    Sigma[2,1] = 0;

    matrix[2,2] Tau;
    Tau[1,1] = tau^2;
    Tau[1,2] = a * tau^2;
    Tau[2,1] = a * tau^2;
    Tau[2,2] = a^2 * tau^2;
}
model {
    for (i in 1:N) {
        y[i, ] ~ multi_normal(Mu, Sigma + Tau);
    }
}
generated quantities {
    vector[2] y_post;
    y_post = multi_normal_rng(Mu, Sigma + Tau);
}
")


sl_fit <- stan(
    model_code = slf_stan,
    data = list(
        N = nrow(dogs),
        lmass = log(dogs$mass),
        lmetabolism = log(dogs$metabolism)
    ),
    chains = 4,
    warmup = 1e3,
    iter = 2e3,
    cores = 4
)
simuls_sl <- rstan::extract(sl_fit)
sl_fit

ggplot() +
    aes(x = simuls_sl$mu) +
    geom_histogram(binwidth = 0.01)

ggplot() +
    aes(x = simuls_sl$sigma) +
    geom_histogram(binwidth = 0.001)

ggplot() +
    aes(x = simuls_sl$tau) +
    geom_histogram(binwidth = 0.005)

ggplot() +
    aes(x = log(dogs$mass), y = log(dogs$metabolism)) +
    geom_point() +
    geom_abline(slope = median(simuls_sl$a), intercept = median(simuls_sl$b))
```

## b

A linear relationship on log scale, on the original scale becomes :

$$
y = e^b x^a
$$

We can compare the estimated trend line with observed data.

```{R}
dogs %>%
    ggplot() +
    aes(x = mass, y = metabolism) +
    geom_point() +
    stat_function(fun = function(x) exp(median(simuls_sl$b)) * x^median(simuls_sl$a))
```

Then we can construct a confidence interval for the trajectory.

```{r}
ci_traj <- tibble(
    x = seq(min(dogs$mass), max(dogs$mass), length.out = 100),
    map_dfr(
        x,
        function(x) {
            y <- exp(simuls_sl$b) * x^simuls_sl$a
            quantile(y, c(0.025, 0.25, 0.5, 0.75, 0.975))
        }
    )
)

ci_traj %>%
    select(-c(down, up)) %>%
    pivot_longer(
        cols = -x,
        names_to = "quantile",
        values_to = "value"
    ) %>%
    ggplot() +
    aes(
        x = x,
        y = value,
        color = quantile,
        group = quantile
    ) +
    geom_line() +
    geom_point(
        aes(x = mass, y = metabolism, color = NULL, group = NULL),
        data = dogs
    )
```

To get a predictive confidence interval we simulate datapoints from the
posterior predictive distribution.

```{r}
ggplot() +
    geom_point(
        aes(x = exp(simuls_sl$y_post[, 1]), y = exp(simuls_sl$y_post[, 2])),
    )
```

Note the very large values that are generated. Indeed a mass of 10e3 kg
seems execive for a dog. This is the concequence of essentially modeling the
mass as a log-normal distribution giving way for some potentially very high
values.

We can still take a look at the relationship for more credible values.

```{r}
ggplot() +
    geom_point(
        aes(x = exp(simuls_sl$y_post[, 1]), y = exp(simuls_sl$y_post[, 2])),
    ) +
    coord_cartesian(
        xlim = c(0, max(dogs$mass)), ylim = c(0, max(dogs$metabolism))
    )
```

We can construct a predictive confidence interval.

```{r}
ci_traj2 <- tibble(
    x = seq(0, max(dogs$mass), length.out = 100),
    map_dfr(
        x,
        function(x) {
            eps <- 0.1
            window <- which(
                x * exp(-eps) < exp(simuls_sl$y_post[, 1]) &
                    exp(simuls_sl$y_post[, 1]) <= x * exp(eps)
            )
            quantile(exp(simuls_sl$y_post[window, 2]), c(0.025, 0.5, 0.975))
        }
    )
)

ci_traj2 %>%
    ggplot() +
    aes(
        x = x,
        y = `50%`,
        ymin = `2.5%`,
        ymax = `97.5%`,
    ) +
    geom_line() +
    geom_ribbon(alpha = 0.05) +
    geom_point(
        aes(x = mass, y = metabolism, ymin = NULL, ymax = NULL),
        data = dogs
    )
```

Overall it appears that the model captures the relationship between mass and
metabolism reasonably well. Although the distribution of the mass, and
metabolism respectively does not seem be particularly well represented as seen
from the unrealisticly large values. 

# c


